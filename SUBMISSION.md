# OttoMarkdown - AI-Powered Document Processing for Ottomator

## Overview
OttoMarkdown is an intelligent document processing agent that enhances Ottomator's capabilities by providing seamless file handling, smart caching, and AI-powered document analysis. Built with FastAPI and integrated with OpenRouter's LLMs, it offers a robust solution for processing and analyzing various document formats.

## Key Features

### 1. Universal Document Processing
- Supports multiple file formats (PDF, DOCX, XLSX, HTML, etc.)
- Intelligent markdown conversion using MarkItDown
- Clean and consistent output formatting

### 2. Smart Caching System
- Document-level caching in Supabase
- Hash-based content tracking
- Efficient retrieval for repeated queries
- Reduced processing time and API costs

### 3. AI-Powered Analysis
- Integration with OpenRouter's meta-llama/llama-3.2-3b-instruct model
- Context-aware document analysis
- Conversation history integration
- Multi-document processing in a single request

### 4. Production-Ready Architecture
- FastAPI for high-performance async operations
- Docker containerization
- Comprehensive error handling
- Detailed logging and monitoring
- Authentication and security measures

## Installation

1. Clone the repository:
```bash
git clone https://github.com/bacoco/ottomarkdown.git
cd ottomarkdown
```

2. Set up environment variables:
```bash
# Supabase configuration
SUPABASE_URL=your_supabase_url
SUPABASE_SERVICE_KEY=your_supabase_service_key

# API authentication
API_BEARER_TOKEN=your_bearer_token

# OpenRouter configuration
OPENAI_API_KEY=your_openrouter_api_key
OPENROUTER_MODEL=meta-llama/llama-3.2-3b-instruct:free
```

3. Run with Docker:
```bash
docker build -t ottomarkdown .
docker run -p 8001:8001 --env-file .env ottomarkdown
```

## Usage Example

```python
import requests
import base64

# Read file
with open("document.pdf", "rb") as f:
    base64_content = base64.b64encode(f.read()).decode()

# Process with AI
response = requests.post(
    "http://localhost:8001/api/file-agent-cached",
    headers={"Authorization": "Bearer your_token"},
    json={
        "query": "Summarize the key points of this document",
        "files": [{
            "name": "document.pdf",
            "type": "application/pdf",
            "base64": base64_content
        }],
        "session_id": "demo-session",
        "user_id": "demo-user",
        "request_id": "demo-request"
    }
)

print(response.json()["markdown"])
```

## Technical Details

### Architecture
```
┌─────────────┐     ┌──────────┐     ┌───────────┐
│ FastAPI App │ ─── │ Supabase │ ─── │ OpenRouter│
└─────────────┘     └──────────┘     └───────────┘
       │                  │                │
       │            ┌──────────┐          │
       └────────── │ Document  │ ─────────┘
                   │  Cache    │
                   └──────────┘
```

### Performance
- Average response time: ~2s for cached documents
- ~5-10s for new document processing
- Supports concurrent requests
- Efficient memory usage with stream processing

## Technical Implementation

### Document Processing Pipeline
```
Input File → Base64 Decode → MarkItDown Conversion → Cache Check/Store → AI Processing → Response
```

1. **File Handling**
   - Base64 decoding for binary files
   - Temporary file management
   - Multiple format support (PDF, DOCX, XLSX, HTML)

2. **Caching System**
   ```python
   # Hash generation for documents
   async def get_document_hash(file_data: Dict[str, Any]) -> str:
       content = file_data.get('base64', '')
       name = file_data.get('name', '')
       return hashlib.sha256(f"{content}{name}".encode()).hexdigest()
   ```

3. **AI Integration**
   ```python
   # Initialize MarkItDown with OpenRouter
   md = MarkItDown(
       llm_client=openai_client,
       llm_model="meta-llama/llama-3.2-3b-instruct:free"
   )
   ```

4. **Database Schema**
   ```sql
   create table if not exists document_cache (
       id bigint generated by default as identity primary key,
       doc_hash text unique not null,
       markdown_content text not null,
       metadata jsonb,
       created_at timestamp with time zone default timezone('utc'::text, now())
   );
   ```

### Performance Metrics
- Average response time: ~2s (cached)
- Memory usage: <100MB base
- Concurrent requests: Up to 100/minute
- Cache hit ratio: >90% for repeated docs

### Security Features
1. Bearer token authentication
2. Input validation
3. Secure file handling
4. Rate limiting
5. Error handling

## Testing

Run the comprehensive test suite:
```bash
python test_markdown.py
```

Example test output:
```
INFO:__main__:Starting API tests...
INFO:__main__:Processing: test.docx
INFO:__main__: Markdown conversion: Success
INFO:__main__: AI processing: Success
INFO:__main__: Cached processing: First: Success, Second: Success
...
```

## API Documentation

### Endpoints

1. `/api/convert-to-markdown`
   - Direct file to markdown conversion
   - No AI processing
   - Fast response time

2. `/api/file-agent`
   - Full AI processing
   - Conversation history
   - Multi-file support

3. `/api/file-agent-cached`
   - Cached processing
   - Smart retrieval
   - Persistent storage

### Request Format
```json
{
    "query": "Analyze this document",
    "files": [{
        "name": "document.pdf",
        "type": "application/pdf",
        "base64": "base64_content"
    }],
    "session_id": "unique_session",
    "user_id": "user123",
    "request_id": "req123"
}
```

### Response Format
```json
{
    "success": true,
    "markdown": "AI-generated analysis...",
    "error": null
}
```

## Dependencies
- FastAPI for API framework
- Supabase for data storage
- OpenRouter for LLM access
- MarkItDown for document processing
- Python 3.11+ for async support

## Deployment
Containerized with Docker for easy deployment:
```bash
docker build -t ottomarkdown .
docker run -p 8001:8001 --env-file .env ottomarkdown
```

## Team
- Developer: [Your Name]
- Built for: Ottomator Hackathon 2025
- Repository: https://github.com/bacoco/ottomarkdown
